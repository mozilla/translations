# https://github.com/browsermt/students/tree/master/train-student/models/student.base
# the difference with the "tiny" configuration is the dimensionality of transformer-dim-ffn and dim-emb
dec-cell-base-depth: 2
dec-cell-high-depth: 1
dec-cell: ssru
dec-depth: 2
dim-emb: 512
enc-cell-depth: 1
enc-cell: gru
enc-depth: 6
enc-type: bidirectional
transformer-decoder-autoreg: rnn
transformer-dim-ffn: 2048
transformer-ffn-activation: relu
transformer-ffn-depth: 2
transformer-guided-alignment-layer: last
transformer-heads: 8
transformer-no-projection: false
transformer-postprocess-emb: d
transformer-postprocess: dan
transformer-preprocess: ""
transformer-tied-layers: []
transformer-train-position-embeddings: false
type: transformer
# tied-embeddings* parameters are applied based on using shared or separate vocabs


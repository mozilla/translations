{
  "batch_size": 1024,
  "langs": ["zh_CN"],
  "max_tok_alpha": 2.0,
  "prompt": "wmt24pp",
  "llm": {
    "max_model_len": 1024,
    "tensor_parallel_size": 4
  },
  "decoding": {
    "temperature": 0.7,
    "top_p": 0.8,
    "top_k": 20,
    "min_p": 0,
    "n": 1
  }
}

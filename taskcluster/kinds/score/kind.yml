# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
---

loader: taskgraph.loader.transform:loader

transforms:
    - translations_taskgraph.transforms.worker_selection
    - taskgraph.transforms.task_context
    - taskgraph.transforms.run:transforms
    - translations_taskgraph.transforms.cached_tasks:transforms
    - taskgraph.transforms.task:transforms

kind-dependencies:
    - train-backwards
    - merge-translated
    - toolchain

tasks:
    "{src_locale}-{trg_locale}":
        description: >
            Compute the cross-entropy score for each sentence in the teacher-synthesized
            distillation data. This dataset is used for student training. This score is
            the cross-entropy cost of the translation as computed by the backtranslations
            model. This represents a probability that the synthesized sentence could be
            generated by the target sentence.
        attributes:
            dataset-category: train
            stage: score
            src_locale: "{src_locale}"
            trg_locale: "{trg_locale}"
            cache:
                type: score
                resources:
                    - pipeline/cefilter/score.sh
        task-context:
            from-parameters:
                best_model: training_config.experiment.best-model
                src_locale: training_config.experiment.src
                trg_locale: training_config.experiment.trg
            substitution-fields:
                - fetches.train-backwards
                - description
                - name
                - fetches
                - dependencies
                - worker.env
                - attributes
                - run.command

        worker-type: b-largegpu-largedisk
        worker:
            max-run-time: 2592000
            env:
                # TODO: what should we _actually_ use for the workspace value?
                # and should we centralize this, since it seems to depend on available
                # memory?
                WORKSPACE: "12000"
                # TODO: this needs to be updated, ideally to have the script detect
                # GPUs. it should _always_ be aligned with the # of GPUs on the intsance
                GPUS: "0 1 2 3"
                SRC: "{src_locale}"
                TRG: "{trg_locale}"
            artifacts:
                - name: public/build
                  path: artifacts
                  type: directory
            # 128 happens when cloning this repository fails
            retry-exit-status: [128]

        # Don't run unless explicitly scheduled
        run-on-tasks-for: []

        run:
            using: run-task
            command:
                - bash
                - -c
                - >-
                    export MARIAN=$MOZ_FETCHES_DIR &&
                    find fetches &&
                    $VCS_PATH/pipeline/cefilter/score.sh
                    $TASK_WORKDIR/fetches/final.model.npz.best-{best_model}.npz
                    $TASK_WORKDIR/fetches/vocab.{src_locale}.spm
                    $TASK_WORKDIR/fetches/vocab.{trg_locale}.spm
                    $TASK_WORKDIR/fetches/corpus
                    $TASK_WORKDIR/artifacts/scores.txt

        dependencies:
            train-backwards: train-backwards-{src_locale}-{trg_locale}
            merge-translated: merge-translated-{src_locale}-{trg_locale}

        fetches:
            toolchain:
                - marian
            train-backwards:
                - artifact: final.model.npz.best-{best_model}.npz
                  extract: false
                - artifact: vocab.{src_locale}.spm
                  extract: false
                - artifact: vocab.{trg_locale}.spm
                  extract: false
            merge-translated:
                - artifact: corpus.{src_locale}.zst
                  extract: false
                - artifact: corpus.{trg_locale}.zst
                  extract: false

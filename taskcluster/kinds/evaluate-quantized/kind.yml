# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
---

loader: taskgraph.loader.transform:loader

transforms:
    - translations_taskgraph.transforms.continuation
    - translations_taskgraph.transforms.from_datasets:per_dataset
    - translations_taskgraph.transforms.worker_selection
    - taskgraph.transforms.task_context
    - taskgraph.transforms.run:transforms
    - translations_taskgraph.transforms.cached_tasks:transforms
    - taskgraph.transforms.task:transforms

kind-dependencies:
    - dataset
    - build-vocab
    - distillation-student-model-quantize
    - distillation-corpus-build-shortlist
    - continuation-vocab
    - toolchain

tasks:
    "{provider}-{dataset_sanitized}-{src_locale}-{trg_locale}":
        description: >
            Evaluate the final quantized student model using a devset. This generates a
            COMET, bleu, and chrF score. The evaluation uses a shortlist.
        attributes:
            stage: evaluate-quantized
            dataset-category: test
            cache:
                type: evaluate-quantized
                resources:
                    - pipeline/eval/eval.py

        dataset-config:
            category: test
            substitution-fields:
                - description
                - name
                - dependencies
                - fetches
                - worker.env
                - task-context
                - run.command
        task-context:
            substitution-fields:
                - run.command
                - fetches
                - worker.env
            from-parameters:
                src_locale: training_config.experiment.src
                trg_locale: training_config.experiment.trg
                wandb_publication: training_config.wandb-publication
                owner: owner

        worker-type: b-gpu
        worker:
            docker-image: {"in-tree": "train"}
            volumes:
                - /builds/worker/artifacts
            artifacts:
                - name: public/build
                  path: /builds/worker/artifacts
                  type: volume
            max-run-time: 2592000
            env:
                # This is a separate environment variable so tests can override it.
                BMT_MARIAN: $MOZ_FETCHES_DIR

                # Weight & Biases trigger
                WANDB_PUBLICATION: "{wandb_publication}"
                WANDB_AUTHOR: "{owner}"

                # Weight & Biases publication token is stored in that secret
                TASKCLUSTER_SECRET: project/translations/level-1/weights-and-biases

            # Taskcluster proxy is required to read secrets
            taskcluster-proxy: true
            # 128 happens when cloning this repository fails
            # 75 - EX_TEMPFAIL, used for when the GPUs aren't available on the machine.
            retry-exit-status: [128, 75]

        # The task needs to be able to read that secret to publish on Weight & Biases
        scopes:
          - secrets:get:project/translations/level-1/weights-and-biases

        # Don't run unless explicitly scheduled
        run-on-tasks-for: []

        run:
            using: run-task
            use-caches: [checkout, pip]
            command:
                - bash
                - -c
                - >-
                    export PATH=$PATH:~/.local/bin &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:$MOZ_FETCHES_DIR/cuda-toolkit/lib64" &&
                    pip install --upgrade pip &&
                    pip install -r $VCS_PATH/pipeline/eval/requirements/eval.txt &&
                    pip install $VCS_PATH/tracking &&
                    zstd --rm -d $MOZ_FETCHES_DIR/lex.s2t.pruned.zst &&
                    $VCS_PATH/pipeline/eval/eval.py
                    --src               {src_locale}
                    --trg               {trg_locale}
                    --models            "$MOZ_FETCHES_DIR/model.intgemm.alphas.bin"
                    --dataset_prefix    "$MOZ_FETCHES_DIR/{dataset_sanitized}"
                    --vocab_src          "$MOZ_FETCHES_DIR/vocab.{src_locale}.spm"
                    --vocab_trg          "$MOZ_FETCHES_DIR/vocab.{trg_locale}.spm"
                    --shortlist         "$MOZ_FETCHES_DIR/lex.s2t.pruned"
                    --artifacts_prefix  "$TASK_WORKDIR/artifacts/{dataset_sanitized}"
                    --marian_config     "$VCS_PATH/pipeline/quantize/decoder.yml"
                    --marian            "$BMT_MARIAN"
                    --gpus              "$GPUS"
                    --model_variant     quantized

        dependencies:
            dataset: dataset-{provider}-{dataset_sanitized}-{src_locale}-{trg_locale}
            build-vocab: build-vocab-{src_locale}-{trg_locale}
            distillation-student-model-quantize: distillation-student-model-quantize-{src_locale}-{trg_locale}
            distillation-corpus-build-shortlist: distillation-corpus-build-shortlist-{src_locale}-{trg_locale}
        fetches:
            dataset:
                - artifact: "{dataset_sanitized}.{src_locale}.zst"
                  extract: false
                - artifact: "{dataset_sanitized}.{trg_locale}.zst"
                  extract: false
            build-vocab:
                - artifact: vocab.{src_locale}.spm
                  extract: false
                - artifact: vocab.{trg_locale}.spm
                  extract: false
            distillation-student-model-quantize:
                - artifact: model.intgemm.alphas.bin
            distillation-corpus-build-shortlist:
                - artifact: lex.s2t.pruned.zst
            toolchain:
                # Quantized models are only supported via the browsermt fork of Marian.
                # https://github.com/browsermt/marian-dev
                - browsermt-marian
                - cuda-toolkit

# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
---

loader: taskgraph.loader.transform:loader

transforms:
    - translations_taskgraph.transforms.continuation
    - translations_taskgraph.transforms.marian_args:transforms
    - translations_taskgraph.transforms.worker_selection
    - taskgraph.transforms.task_context
    - taskgraph.transforms.run:transforms
    - translations_taskgraph.transforms.cached_tasks:transforms
    - taskgraph.transforms.task:transforms

kind-dependencies:
    - corpus-merge-devset
    - distillation-student-model-train
    - build-vocab
    - corpus-align-distillation
    - continuation-vocab
    - continuation-corpus
    - toolchain

tasks:
    "{src_locale}-{trg_locale}":
        description: >
            Finetune the student model on simulated quantized values. This improves the
            quality for the final quantization step.
        task-context:
            from-parameters:
                src_locale: training_config.experiment.src
                trg_locale: training_config.experiment.trg
                best_model: training_config.experiment.best-model
                student_model: training_config.experiment.student-model
                wandb_publication: training_config.wandb-publication
                owner: owner
            substitution-fields:
                - description
                - name
                - fetches
                - dependencies
                - fetches.distillation-student-model-train
                - run.command
                - attributes
                - worker.env
        attributes:
            stage: distillation-student-model-finetune
            src_locale: "{src_locale}"
            trg_locale: "{trg_locale}"
            cache:
                type: distillation-student-model-finetune
                resources:
                    - pipeline/train/configs/model/student.{student_model}.yml
                    - pipeline/train/configs/opustrainer/student.yml
                    - pipeline/train/configs/opustrainer/student.cjk.yml
                    - pipeline/train/configs/training/student.train.yml
                    - pipeline/train/train.py
                    - taskcluster/scripts/pipeline/train_taskcluster.py
                    - taskcluster/scripts/pipeline/train-taskcluster.sh
                from-parameters:
                    marian_args: training_config.marian-args.training-student-finetuned

        # Don't run unless explicitly scheduled
        run-on-tasks-for: []

        worker-type: b-largegpu-xxlargedisk
        worker:
            docker-image: {"in-tree": "train"}
            max-run-time: 2592000
            # train_taskcluster.py exits with 17 if a request to Taskcluster fails
            # 75 - EX_TEMPFAIL, used for when the GPUs aren't available on the machine.
            # 128 happens when cloning this repository fails
            retry-exit-status: [17, 75, 128]
            env:
                # Weight & Biases trigger
                WANDB_PUBLICATION: "{wandb_publication}"
                WANDB_AUTHOR: "{owner}"

                # Weight & Biases publication token is stored in that secret
                TASKCLUSTER_SECRET: project/translations/level-1/weights-and-biases
            volumes:
                - /builds/worker/artifacts
            artifacts:
                - name: public/build
                  path: /builds/worker/artifacts
                  type: volume

            # Taskcluster proxy is required to read secrets
            taskcluster-proxy: true

        # The task needs to be able to read that secret to publish on Weight & Biases
        scopes:
          - secrets:get:project/translations/level-1/weights-and-biases

        marian-args:
            from-parameters: training_config.marian-args.training-student-finetuned
        run:
            using: run-task
            use-caches: [checkout, pip]
            command:
                - bash
                - -cx
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/train/requirements/train.txt &&
                    pip3 install $VCS_PATH/tracking &&
                    export PATH="$HOME/.local/bin:$PATH" &&
                    export MARIAN=$MOZ_FETCHES_DIR &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:$MOZ_FETCHES_DIR/cuda-toolkit/lib64" &&
                    $VCS_PATH/taskcluster/scripts/pipeline/train_taskcluster.py
                    student
                    finetune
                    {src_locale}
                    {trg_locale}
                    $MOZ_FETCHES_DIR/corpus.tok-icu
                    $MOZ_FETCHES_DIR/devset
                    $TASK_WORKDIR/artifacts
                    {best_model}
                    $MOZ_FETCHES_DIR/corpus.aln.zst
                    0
                    None
                    {student_model}
                    None
                    None
                    --pretrained-model
                    $MOZ_FETCHES_DIR/final.model.npz.best-{best_model}.npz
                    {marian_args}

        dependencies:
            build-vocab: build-vocab-{src_locale}-{trg_locale}
            corpus-merge-devset: corpus-merge-devset-{src_locale}-{trg_locale}
            distillation-student-model-train: distillation-student-model-train-{src_locale}-{trg_locale}
            corpus-align-distillation: corpus-align-distillation-{src_locale}-{trg_locale}

        fetches:
            toolchain:
                - marian
                - cuda-toolkit
            build-vocab:
                - artifact: vocab.{src_locale}.spm
                  extract: false
                - artifact: vocab.{trg_locale}.spm
                  extract: false
            corpus-merge-devset:
                - artifact: devset.{src_locale}.zst
                  extract: false
                - artifact: devset.{trg_locale}.zst
                  extract: false
            distillation-student-model-train:
                - artifact: final.model.npz.best-{best_model}.npz
                  extract: false
            corpus-align-distillation:
                - artifact: corpus.aln.zst
                - artifact: corpus.tok-icu.{src_locale}.zst
                  extract: false
                - artifact: corpus.tok-icu.{trg_locale}.zst
                  extract: false

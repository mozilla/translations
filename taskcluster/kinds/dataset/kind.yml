# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# This kind primarily exists because these dataset fetches break
# some assumptions made the `job` transforms that treat the `fetch`
# kind specially.
---
loader: taskgraph.loader.transform:loader

transforms:
    - translations_taskgraph.transforms.continuation
    - translations_taskgraph.transforms.from_datasets:per_dataset
    - translations_taskgraph.transforms.worker_selection
    - taskgraph.transforms.task_context
    - taskgraph.transforms.run:transforms
    - translations_taskgraph.transforms.cached_tasks:transforms
    - taskgraph.transforms.task:transforms

task-defaults:
    attributes:
        cache:
            type: dataset
    dataset-config:
        substitution-fields:
            - name
            - label
            - worker.env
            - run.command
    task-context:
        substitution-fields: []
    worker:
        chain-of-trust: true
        docker-image: {in-tree: toolchain-build}
        max-run-time: 86400
        env:
            SRC: "{src_locale}"
            TRG: "{trg_locale}"
        volumes:
            - /builds/worker/artifacts
        artifacts:
            - name: public/build
              path: /builds/worker/artifacts
              type: volume
        # 128 happens when cloning this repository fails
        retry-exit-status: [128]

    run-on-tasks-for: []
    run:
        using: run-task

tasks:
    flores:
        description: Fetch a flores101 dataset.
        label: dataset-flores-{dataset_sanitized}-{src_locale}-{trg_locale}
        worker-type: b-cpu
        dataset-config:
            provider: flores
        attributes:
            cache:
                resources:
                    - pipeline/data/parallel_downloaders.py
                    - pipeline/data/parallel_importer.py
                    - pipeline/data/requirements/data.txt
                    - pipeline/data/cjk.py
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/parallel_importer.py
                    --dataset {dataset}
                    --output_prefix $TASK_WORKDIR/artifacts/{dataset_sanitized}
                    --src {src_locale}
                    --trg {trg_locale}

    sacrebleu:
        description: Fetch a sacrebleu dataset.
        label: dataset-sacrebleu-{dataset_sanitized}-{src_locale}-{trg_locale}
        worker-type: b-cpu
        dataset-config:
            provider: sacrebleu
        attributes:
            cache:
                resources:
                    - pipeline/data/parallel_downloaders.py
                    - pipeline/data/parallel_importer.py
                    - pipeline/data/requirements/data.txt
                    - pipeline/data/cjk.py
        run:
            command:
                - bash
                - -cx
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 -u $VCS_PATH/pipeline/data/parallel_importer.py
                    --dataset {dataset}
                    --output_prefix $TASK_WORKDIR/artifacts/{dataset_sanitized}
                    --src {src_locale}
                    --trg {trg_locale}

    opus:
        description: Fetch a opus dataset.
        # No slashes version of dataset used here because slashes break caches
        label: dataset-opus-{dataset_sanitized}-{src_locale}-{trg_locale}
        worker-type: b-cpu-largedisk
        dataset-config:
            provider: opus
        attributes:
            cache:
                resources:
                    - pipeline/data/parallel_downloaders.py
                    - pipeline/data/parallel_importer.py
                    - pipeline/data/requirements/data.txt
                    - pipeline/data/cjk.py
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/parallel_importer.py
                    --dataset {dataset}
                    --output_prefix $TASK_WORKDIR/artifacts/{dataset_sanitized}
                    --src {src_locale}
                    --trg {trg_locale}

    url:
        description: Fetch a parallel corpus from a URL.
        # No slashes version of dataset used here because slashes break caches
        label: dataset-url-{dataset_sanitized}-{src_locale}-{trg_locale}
        worker-type: b-cpu-largedisk
        dataset-config:
            provider: url
        attributes:
            cache:
                resources:
                    - pipeline/data/parallel_downloaders.py
                    - pipeline/data/parallel_importer.py
                    - pipeline/data/requirements/data.txt
                    - pipeline/data/cjk.py
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/parallel_importer.py
                    --dataset {dataset}
                    --output_prefix $TASK_WORKDIR/artifacts/{dataset_sanitized}
                    --src {src_locale}
                    --trg {trg_locale}

    tmx:
        description: Fetch a parallel corpus from a TMX predefined URL.
        label: dataset-tmx-{dataset_sanitized}-{src_locale}-{trg_locale}
        worker-type: b-cpu
        dataset-config:
            provider: tmx
        attributes:
            cache:
                resources:
                    - pipeline/data/parallel_downloaders.py
                    - pipeline/data/parallel_importer.py
                    - pipeline/data/requirements/data.txt
                    - pipeline/data/cjk.py
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/parallel_importer.py
                    --dataset {dataset}
                    --output_prefix $TASK_WORKDIR/artifacts/{dataset_sanitized}
                    --src {src_locale}
                    --trg {trg_locale}

    url-{src_locale}:
        description: Fetch a monolingual corpus from a URL for {src_locale}.
        label: dataset-url-{dataset_sanitized}-{src_locale}
        worker-type: b-cpu-largedisk
        dataset-config:
            provider: url
            category: mono-src
        attributes:
            cache:
                resources:
                    - pipeline/data/mono_importer.py
                    - pipeline/data/cjk.py
                from-parameters:
                    max_sentences:
                        - training_config.experiment.mono-max-sentences-src.per-dataset
        task-context:
            from-parameters:
                max_sentences:
                    - training_config.experiment.mono-max-sentences-src.per-dataset
            substitution-fields:
                - run.command
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/mono_importer.py
                    --dataset {dataset}
                    --language {src_locale}
                    --src {src_locale}
                    --trg {trg_locale}
                    --max_sentences {max_sentences}
                    --artifacts $TASK_WORKDIR/artifacts

    url-{trg_locale}:
        description: Fetch a monolingual corpus from a URL for {trg_locale}.
        label: dataset-url-{dataset_sanitized}-{trg_locale}
        worker-type: b-cpu-largedisk
        dataset-config:
            provider: url
            category: mono-trg
        attributes:
            cache:
                resources:
                    - pipeline/data/mono_importer.py
                    - pipeline/data/cjk.py
                from-parameters:
                    max_sentences:
                        - training_config.experiment.mono-max-sentences-trg.per-dataset
        task-context:
            from-parameters:
                max_sentences:
                    - training_config.experiment.mono-max-sentences-trg.per-dataset
            substitution-fields:
                - run.command
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/mono_importer.py
                    --dataset {dataset}
                    --language {trg_locale}
                    --src {src_locale}
                    --trg {trg_locale}
                    --max_sentences {max_sentences}
                    --artifacts $TASK_WORKDIR/artifacts

    opus-{src_locale}:
        description: Fetch a monolingual opus dataset for {src_locale}.
        label: dataset-opus-{dataset_sanitized}-{src_locale}
        worker-type: b-cpu-largedisk
        dataset-config:
            provider: opus
            category: mono-src
        attributes:
            cache:
                resources:
                    - pipeline/data/mono_importer.py
                    - pipeline/data/cjk.py
                from-parameters:
                    max_sentences:
                        - training_config.experiment.mono-max-sentences-src.per-dataset
        task-context:
            from-parameters:
                max_sentences:
                    - training_config.experiment.mono-max-sentences-src.per-dataset
            substitution-fields:
                - run.command
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/mono_importer.py
                    --dataset {dataset}
                    --language {src_locale}
                    --src {src_locale}
                    --trg {trg_locale}
                    --max_sentences {max_sentences}
                    --artifacts $TASK_WORKDIR/artifacts

    opus-{trg_locale}:
        description: Fetch a monolingual opus dataset for {trg_locale}.
        label: dataset-opus-{dataset_sanitized}-{trg_locale}
        worker-type: b-cpu-largedisk
        dataset-config:
            provider: opus
            category: mono-trg
        attributes:
            cache:
                resources:
                    - pipeline/data/mono_importer.py
                    - pipeline/data/cjk.py
                from-parameters:
                    max_sentences:
                        - training_config.experiment.mono-max-sentences-trg.per-dataset
        task-context:
            from-parameters:
                max_sentences:
                    - training_config.experiment.mono-max-sentences-trg.per-dataset
            substitution-fields:
                - run.command
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/mono_importer.py
                    --dataset {dataset}
                    --language {trg_locale}
                    --src {src_locale}
                    --trg {trg_locale}
                    --max_sentences {max_sentences}
                    --artifacts $TASK_WORKDIR/artifacts

    hplt-{src_locale}:
        description: Fetch a sample of the HPLT monolingual data for {src_locale}.
        label: dataset-hplt-{dataset_sanitized}-{src_locale}
        worker-type: b-cpu-largedisk
        dataset-config:
            provider: hplt
            category: mono-src
        attributes:
            cache:
                resources:
                    - pipeline/data/mono_importer.py
                    - pipeline/data/cjk.py
                from-parameters:
                    max_sentences:
                        - training_config.experiment.mono-max-sentences-src.per-dataset
                    hplt_min_doc_score:
                        - training_config.experiment.hplt-min-doc-score.mono-src
        task-context:
            from-parameters:
                max_sentences:
                    - training_config.experiment.mono-max-sentences-src.per-dataset
                hplt_min_doc_score:
                    - training_config.experiment.hplt-min-doc-score.mono-src
            substitution-fields:
                - run.command
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/mono_importer.py
                    --dataset {dataset}
                    --language {src_locale}
                    --src {src_locale}
                    --trg {trg_locale}
                    --max_sentences {max_sentences}
                    --hplt_min_doc_score {hplt_min_doc_score}
                    --artifacts $TASK_WORKDIR/artifacts

    hplt-{trg_locale}:
        description: Fetch a sample of the HPLT monolingual data for {trg_locale}.
        label: dataset-hplt-{dataset_sanitized}-{trg_locale}
        worker-type: b-cpu-largedisk
        dataset-config:
            provider: hplt
            category: mono-trg
        attributes:
            cache:
                resources:
                    - pipeline/data/mono_importer.py
                    - pipeline/data/cjk.py
                from-parameters:
                    max_sentences:
                        - training_config.experiment.mono-max-sentences-trg.per-dataset
                    hplt_min_doc_score:
                        - training_config.experiment.hplt-min-doc-score.mono-trg

        task-context:
            from-parameters:
                max_sentences:
                    - training_config.experiment.mono-max-sentences-trg.per-dataset
                hplt_min_doc_score:
                    - training_config.experiment.hplt-min-doc-score.mono-trg
            substitution-fields:
                - run.command
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/mono_importer.py
                    --dataset {dataset}
                    --language {trg_locale}
                    --src {src_locale}
                    --trg {trg_locale}
                    --max_sentences {max_sentences}
                    --hplt_min_doc_score {hplt_min_doc_score}
                    --artifacts $TASK_WORKDIR/artifacts

    mtdata:
        description: Fetch a mtdata dataset.
        label: dataset-mtdata-{dataset_sanitized}-{src_locale}-{trg_locale}
        worker-type: b-cpu-largedisk
        dataset-config:
            provider: mtdata
        attributes:
            cache:
                resources:
                    - pipeline/data/parallel_downloaders.py
                    - pipeline/data/parallel_importer.py
                    - pipeline/data/requirements/data.txt
                    - pipeline/data/cjk.py
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/parallel_importer.py
                    --dataset {dataset}
                    --output_prefix $TASK_WORKDIR/artifacts/{dataset_sanitized}
                    --src {src_locale}
                    --trg {trg_locale}

    news-crawl-{src_locale}:
        description: Fetch a news-crawl dataset for {src_locale}.
        label: dataset-news-crawl-{dataset_sanitized}-{src_locale}
        worker-type: b-cpu-largedisk
        dataset-config:
            provider: news-crawl
            category: mono-src
        attributes:
            cache:
                resources:
                    - pipeline/data/mono_importer.py
                    - pipeline/data/cjk.py
                from-parameters:
                    max_sentences:
                        - training_config.experiment.mono-max-sentences-src.per-dataset
        task-context:
            from-parameters:
                max_sentences:
                    - training_config.experiment.mono-max-sentences-src.per-dataset
            substitution-fields:
                - run.command
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/mono_importer.py
                    --dataset {dataset}
                    --language {src_locale}
                    --src {src_locale}
                    --trg {trg_locale}
                    --max_sentences {max_sentences}
                    --artifacts $TASK_WORKDIR/artifacts

    news-crawl-{trg_locale}:
        description: Fetch a news-crawl dataset for {trg_locale}.
        label: dataset-news-crawl-{dataset_sanitized}-{trg_locale}
        worker-type: b-cpu-largedisk
        dataset-config:
            provider: news-crawl
            category: mono-trg
        attributes:
            cache:
                resources:
                    - pipeline/data/mono_importer.py
                    - pipeline/data/cjk.py
                from-parameters:
                    max_sentences:
                        - training_config.experiment.mono-max-sentences-trg.per-dataset
        task-context:
            from-parameters:
                max_sentences:
                    - training_config.experiment.mono-max-sentences-trg.per-dataset
            substitution-fields:
                - run.command
        run:
            command:
                - bash
                - -c
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/data/requirements/data.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/data/mono_importer.py
                    --dataset {dataset}
                    --language {trg_locale}
                    --src {src_locale}
                    --trg {trg_locale}
                    --max_sentences {max_sentences}
                    --artifacts $TASK_WORKDIR/artifacts

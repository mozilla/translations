# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
---

loader: taskgraph.loader.transform:loader

transforms:
    - translations_taskgraph.transforms.continuation
    - translations_taskgraph.transforms.training_continuation:transforms
    - translations_taskgraph.transforms.marian_args:transforms
    - translations_taskgraph.transforms.worker_selection
    - taskgraph.transforms.task_context
    - translations_taskgraph.transforms.cast_to
    - taskgraph.transforms.chunking
    - taskgraph.transforms.run:transforms
    - translations_taskgraph.transforms.cached_tasks:transforms
    - taskgraph.transforms.task:transforms

kind-dependencies:
    - corpus-merge-devset
    - build-vocab
    - corpus-align-parallel
    - corpus-align-backtranslations
    - toolchain
    - continuation-corpus
    - continuation-vocab

tasks:
    # double curly braces are used for the chunk substitutions because
    # this must first be formatted by task-context to get src and trg locale
    "{src_locale}-{trg_locale}-{{this_chunk}}":
        description: >
            Trains a teacher model from a mix of the original cleaned and merged parallel
            datasets, and the synthesized data generated by the back translations model.
            The back translations model generates synthesied target sentences from
            monolingual source sentences (where no parallel sentence is available).
        chunk:
            total-chunks: "{teacher_ensemble}"
            substitution-fields:
                - name
                - description
                - run.command
        cast-to:
            int:
                - chunk.total-chunks
        task-context:
            from-parameters:
                src_locale: training_config.experiment.src
                trg_locale: training_config.experiment.trg
                best_model: training_config.experiment.best-model
                teacher_ensemble: training_config.experiment.teacher-ensemble
                teacher_mode: training_config.experiment.teacher-mode
                pretrained_teacher_mode: training_config.continuation.models.teacher.mode
                pretrained_teacher_type: training_config.continuation.models.teacher.type
                wandb_publication: training_config.wandb-publication
                owner: owner
            substitution-fields:
                - description
                - name
                - fetches
                - dependencies
                - run.command
                - attributes
                - chunk.total-chunks
                - worker.env
        attributes:
            stage: train-teacher-model
            src_locale: "{src_locale}"
            trg_locale: "{trg_locale}"
            best_model: "{best_model}"
            cache:
                type: train-teacher-model
                resources:
                    - pipeline/train/configs/model/teacher.yml
                    - pipeline/train/configs/training/teacher.train.yml
                    - pipeline/train/train.py
                    - taskcluster/scripts/pipeline/train_taskcluster.py
                    - taskcluster/scripts/pipeline/train-taskcluster.sh
                from-parameters:
                    marian_args: training_config.marian-args.training-teacher
                    teacher_mode: training_config.experiment.teacher-mode
                    pretrained_teacher: training_config.continuation.models.train-teacher
        worker-type:
            by-tasks-for:
                # pull requests and nightly pipeline runs use a minimal config
                # that doesn't requires as much disk as full fledged runs
                github-pull-request: b-largegpu
                cron: b-largegpu
                default: b-largegpu-xxlargedisk
        worker:
            chain-of-trust: true
            docker-image: {"in-tree": "train"}
            max-run-time: 2592000
            # train_taskcluster.py exits with 17 if a request to Taskcluster fails
            # 75 - EX_TEMPFAIL, used for when the GPUs aren't available on the machine.
            # 128 happens when cloning this repository fails
            retry-exit-status: [17, 75, 128]

            env:
                # Weight & Biases trigger
                WANDB_PUBLICATION: "{wandb_publication}"
                WANDB_AUTHOR: "{owner}"

                # Weight & Biases publication token is stored in that secret
                TASKCLUSTER_SECRET: project/translations/level-1/weights-and-biases
            volumes:
                - /builds/worker/artifacts
            artifacts:
                - name: public/build
                  path: /builds/worker/artifacts
                  type: volume

            # Taskcluster proxy is required to read secrets
            taskcluster-proxy: true

        # The task needs to be able to read that secret to publish on Weight & Biases
        scopes:
          - secrets:get:project/translations/level-1/weights-and-biases

        # Don't run unless explicitly scheduled
        run-on-tasks-for: []

        marian-args:
            from-parameters: training_config.marian-args.training-teacher
        run:
            using: run-task
            use-caches: [checkout, pip]
            # order of comma separated datasets is important
            command:
                - bash
                - -cx
                - >-
                    pip3 install --upgrade pip setuptools &&
                    pip3 install -r $VCS_PATH/pipeline/train/requirements/train.txt &&
                    pip3 install $VCS_PATH/tracking &&
                    export PATH="$HOME/.local/bin:$PATH" &&
                    export MARIAN=$MOZ_FETCHES_DIR &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:$MOZ_FETCHES_DIR/cuda-toolkit/lib64" &&
                    $VCS_PATH/taskcluster/scripts/pipeline/train_taskcluster.py
                    teacher
                    train
                    {src_locale}
                    {trg_locale}
                    $MOZ_FETCHES_DIR/corpus.tok-icu,$MOZ_FETCHES_DIR/mono.tok-icu
                    $MOZ_FETCHES_DIR/devset
                    $TASK_WORKDIR/artifacts
                    {best_model}
                    $MOZ_FETCHES_DIR/corpus.aln.zst,$MOZ_FETCHES_DIR/mono.aln.zst
                    {{this_chunk}}
                    {teacher_mode}
                    None
                    {pretrained_teacher_mode}
                    {pretrained_teacher_type}
                    {marian_args}

        dependencies:
            build-vocab: build-vocab-{src_locale}-{trg_locale}
            corpus-merge-devset: corpus-merge-devset-{src_locale}-{trg_locale}
            corpus-align-parallel: corpus-align-parallel-{src_locale}-{trg_locale}
            corpus-align-backtranslations: corpus-align-backtranslations-{src_locale}-{trg_locale}

        fetches:
            toolchain:
                - marian
                - cuda-toolkit
            build-vocab:
                - artifact: vocab.{src_locale}.spm
                  extract: false
                - artifact: vocab.{trg_locale}.spm
                  extract: false
            corpus-merge-devset:
                - artifact: devset.{src_locale}.zst
                  extract: false
                - artifact: devset.{trg_locale}.zst
                  extract: false
            corpus-align-parallel:
                - artifact: corpus.aln.zst
                - artifact: corpus.tok-icu.{src_locale}.zst
                  extract: false
                - artifact: corpus.tok-icu.{trg_locale}.zst
                  extract: false
            corpus-align-backtranslations:
                - artifact: mono.aln.zst
                - artifact: mono.tok-icu.{trg_locale}.zst
                  extract: false
                - artifact: mono.tok-icu.{src_locale}.zst
                  extract: false

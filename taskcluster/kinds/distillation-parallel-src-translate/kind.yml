# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
---
loader: taskgraph.loader.transform:loader

transforms:
    - translations_taskgraph.transforms.continuation
    - translations_taskgraph.transforms.marian_args:transforms
    - translations_taskgraph.transforms.worker_selection
    - taskgraph.transforms.task_context
    - translations_taskgraph.transforms.cast_to
    - taskgraph.transforms.chunking
    - taskgraph.transforms.from_deps
    - taskgraph.transforms.run:transforms
    - translations_taskgraph.transforms.cached_tasks:transforms
    - taskgraph.transforms.task:transforms

kind-dependencies:
    - distillation-parallel-src-chunk
    - train-teacher-model
    - toolchain

tasks:
    # double curly braces are used for the chunk substitutions because
    # this must first be formatted by task-context to get src and trg locale
    "{src_locale}-{trg_locale}-{{this_chunk}}/{{total_chunks}}":
        description: >
            Translate the original source side of the parallel data with the teacher model
            in order to synthesize distillation data for the student model to train from.
            This uses the nbest option so that multiple translations are generated for
            each source sentence. This option requires there to be a reference translation
            so it only works with the original parallel data. See
            "distillation-parallel-src-extract-best" for how this data is consolidated.
        attributes:
            stage: distillation-parallel-src-translate
            src_locale: "{src_locale}"
            trg_locale: "{trg_locale}"
            dataset-category: train
            cache:
                type: distillation-parallel-src-translate
                resources:
                    - pipeline/translate/translate.py
                from-parameters:
                    split_chunks: training_config.taskcluster.split-chunks
                    marian_args: training_config.marian-args.decoding-teacher
                    teacher_decoder: training_config.experiment.teacher-decoder

        # This job is split into `split-chunks`
        chunk:
            total-chunks: "{split_chunks}"
            substitution-fields:
                - name
                - run.command
                - attributes
                - from-deps.fetches.distillation-parallel-src-chunk

        cast-to:
            int:
                - chunk.total-chunks

        from-deps:
            group-by: all
            set-name: null
            unique-kinds: false
            kinds:
                - train-teacher-model
                - distillation-parallel-src-chunk
            fetches:
                distillation-parallel-src-chunk:
                    - artifact: file.{this_chunk}.zst
                      extract: true
                train-teacher-model:
                    - artifact: final.model.npz.best-{best_model}.npz
                      dest: model{this_chunk}
                      extract: false
                    - artifact: vocab.{src_locale}.spm
                      extract: false
                    - artifact: vocab.{trg_locale}.spm
                      extract: false

        task-context:
            from-parameters:
                src_locale: training_config.experiment.src
                trg_locale: training_config.experiment.trg
                best_model: training_config.experiment.best-model
                split_chunks: training_config.taskcluster.split-chunks
                teacher_decoder: training_config.experiment.teacher-decoder
            substitution-fields:
                - description
                - worker.env
                - name
                - run.command
                - chunk.total-chunks
                - attributes

        marian-args:
            from-parameters: training_config.marian-args.decoding-teacher

        # Don't run unless explicitly scheduled
        run-on-tasks-for: []

        worker-type: b-largegpu-xlargedisk
        worker:
            max-run-time: 2592000
            artifacts:
                - name: public/build
                  path: artifacts
                  type: directory
            env:
                CUDA_DIR: fetches/cuda-toolkit
                CUDNN_DIR: fetches/cuda-toolkit
                MARIAN: $MOZ_FETCHES_DIR
            # 75 - EX_TEMPFAIL, used for when the GPUs aren't available on the machine.
            # 128 happens when cloning this repository fails
            retry-exit-status: [75, 128]

        run:
            using: run-task
            use-caches: [checkout, pip]
            command:
                - bash
                - -xc
                # double curly braces are used for the chunk substitutions because
                # this must first be formatted by task-context to get src and trg locale
                # upgrade pip to work around an issue with gpustat/setuptools-scm
                # this can be reverted if either gpustat or setuptools-scm works around
                # the issue, or if we upgrade to ubuntu 24.04.
                # see https://github.com/mozilla/translations/issues/1044
                # (this is done here to avoid rebuilding worker images for what
                # is hopefully a short term issue.)
                - >-
                    pip3 install -U pip==25.0.1 &&
                    pip3 install -r $VCS_PATH/pipeline/translate/requirements/translate-ctranslate2.txt &&
                    export PYTHONPATH=$PYTHONPATH:$VCS_PATH &&
                    python3 $VCS_PATH/pipeline/translate/translate.py
                    --input       "$MOZ_FETCHES_DIR/file.{{this_chunk}}.zst"
                    --models_glob "$MOZ_FETCHES_DIR/*.npz" "$MOZ_FETCHES_DIR/model*/*.npz"
                    --artifacts   "$TASK_WORKDIR/artifacts"
                    --vocab_src   "$MOZ_FETCHES_DIR/vocab.{src_locale}.spm"
                    --vocab_trg   "$MOZ_FETCHES_DIR/vocab.{trg_locale}.spm"
                    --marian_dir  "$MARIAN"
                    --gpus        "$GPUS"
                    --workspace   "$WORKSPACE"
                    --decoder     "{teacher_decoder}"
                    --nbest
                    --
                    {marian_args}

        fetches:
            toolchain:
                - marian
                - cuda-toolkit
